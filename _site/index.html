<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title> Lucid</title>
<meta name="description" content="">
<meta name="author" content="">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="/lucid/css/main.css">
<script type="text/javascript" src="//use.typekit.net/xay3ywg.js"></script>
<script type="text/javascript">try{Typekit.load();}catch(e){}</script>

<!--[if lt IE 9]><script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
</head>
<body>

	<header class="global-header" role="banner">

		<a href="http://sambillingham.github.com/lucid"><h1>Lucid</h1></a>

	</header>

	<div class="content">

		<section class="main wrap" role="main">

			<ul>

	

		

			<article class="post">

				<header>

					<h2><a class="title" href="/lucid/the-challenge">The Challenge</a></h2>

					<div class="meta">

						<span class="date">22 February 2013</span>

						<span class="author">by Flo</span>

					</div>

				</header>

				<section class="words">

					<p>Testy test test</p>


				</section>

			</article>

		

			<article class="post">

				<header>

					<h2><a class="title" href="/lucid/lucid-app">Lucid.app</a></h2>

					<div class="meta">

						<span class="date">21 February 2013</span>

						<span class="author">by Ben</span>

					</div>

				</header>

				<section class="words">

					<p>As mentioned in previous posts, Lucid revolves around the exploration and experimentation in the field of Hypnopaedia – or  sleep-learning.</p>

<p>From the early stages of development, we knew that we would need some kind of device that could be used to track the varying sleep cycles that a user will go through nightly. Our initial idea was to use an Arduino attached to various sensors such as accelerometers and microphones. We eventually realised that this solution would result in a bulky and impractical product, and began to look elsewhere.</p>

<p>Luckily, the iPhone 4S in my left pocket happened to support every feature that we wished to implement – accelerometer, microphone, speaker – and it did it all in a beautiful little package.</p>

<p>So, we decided to build an iOS app, allowing us to access all of the native hardware features that we would require for our project.</p>

<p>As outlined in 'Dreams', the aim of the app was simple: Detect when a user is in a state of REM sleep, and begin to play them audio. The user will then be subjected to a 'test' at a later date to determine whether or not they absorbed the information played to them whilst they slept.</p>

<h2>REM Detection</h2>

<p>The first step of the 'injection' process was to determine when a user was in REM sleep. Apps such as Sleep Cycle achieve this by waiting for complete stillness – one of the traits of sleep paralysis associated with REM sleep. We achieved this is a very similar way. Lucid will start a 30 minute timer when activated and if the timer is uninterruped by movement, we know it is likely that the user is in REM sleep.</p>

<p>As soon as the user enters back in to a lighter state of sleep, however, the timer will be interrupted, reset back to zero and the process is restarted all over again. This process is likely to occur 3-5 timer per night.</p>

<h2>Playing Audio</h2>

<p>Now that we know our user is in REM sleep, we are ready to begin sending them audio. The 'test' involved a user correctly solving three puzzles: One a simple deduction puzzle, the next a descriptive stoytelling task, and the final a musical composition challenge.</p>

<p>Lucid will play clues during a user's REM cycles that relate directly to these puzzles, hopefully ensuring that the user will be able to complete them successfully when faced.</p>

<h2>An alarm clock with a twist</h2>

<p>Now, we had a fully functioning app, ready and waiting to inject thoughts in to those willing to participate, but we wanted the app to appear as though it was nothing more than just an alarm clock.</p>

<p>On top of these features, we added all the basic functionality ofma standard alarm clock, allowing for the user to use the app on a daily basis without suspecting anything.</p>


				</section>

			</article>

		

			<article class="post">

				<header>

					<h2><a class="title" href="/lucid/the-immersive-environment">The Immersive Environment</a></h2>

					<div class="meta">

						<span class="date">17 February 2013</span>

						<span class="author">by Sam</span>

					</div>

				</header>

				<section class="words">

					<p><em>"Participants within our experiment are invited to take part in the 'testing' phase whenever they feel ready to do so."</em></p>

<p>The ability to implant thoughts into a participants mind is the main focus for our experiment. With the scepticism behind Hypnopaedia we decided to help our participants along by making the environment for recall as immersive as possible, with the hope than recall would be improved. The testing phase will be conducted within the immersive vision theatre at Plymouth University, which is home to a hemispherical dome, tilted audience seating and an atmospheric environment. We hope that the increased peripheral vision and enhanced audio visual experience will help participants to recall the information from their sleeping state for use within our experimental arena.</p>

<h2>Enhancing the Interface</h2>

<p>Each participant will need 3 tools to fully interact with our experiments, 2 iPhones and a NeuroSky MindWave (All are provided). While the phones are used to allow a user to control their experience the MindWave device is intentionally added to help a user become more aware of specific states they're currently in.</p>

<p>We employ functionality with the NeuroSky Mindwave to control a to blurring effect within the experiment, this blur effect is in direct correlation to a participants relaxation state and diminishes as the subject becomes more relaxed. We choose this simple blur effect to act as a feedback loop for the subject allowing them to constantly asses how relaxed they are.</p>

<h2>Player Controls</h2>

<p>Early on in this project we transitioned from a co-op experience to a single player one, this left us with a detached controller  model aimed at two players working together to achieve movement and view. Instead of throwing our controls into a gamepad, we decided to keep our controls split and allow the player to use both at the same time. Both Movement and Look are based around using an iPhone's gyroscope and accelerometer.</p>

<p>Movement is via holding the phone as it acts like a controller, Tilting forward and backward moves a subject in the respective direction and tiling left and right rotates the subject to allow multi directional movement.</p>

<p>The ability to look around is slightly different and involves an iPhone attached to the MindWave sensor situated on a subjects head. The act of attaching an iPhone to a subject head may seem ominous or unnecessary, however it adds an interesting element of natural control. The iPhone controls where the character within Unity is currently looking, there is such an extreme field of view within the IVT that this becomes a subtle movement. While naturally looking at different aspects in the environment the centre position slightly changes in reaction to the subject moving their head.</p>

<h2>Cogs in the machine</h2>

<p>To achieve the interaction between the mobile accelerometer for our controller and the experiment created within Unity 3D we used a number of real-time web technologies. Using Node.js, Socket.io and Node-OSC we were able to create seamless interaction and enhance our project with interaction devices of our choosing. All of the code behind our project is publicly available in our Github repo, <a href="https://github.com/Sambillingham/lucid">here</a>.</p>


				</section>

			</article>

		

			<article class="post">

				<header>

					<h2><a class="title" href="/lucid/dreams">Dreams</a></h2>

					<div class="meta">

						<span class="date">13 February 2013</span>

						<span class="author">by Ben</span>

					</div>

				</header>

				<section class="words">

					<p>In recent months, the iOS App Store and Google Play have seen an influx of sleep analysis applications that allow a user to monitor and analyse numerous aspects of their everyday sleeping habits, including duration, the amount of time spent in light sleep vs. REM sleep and the overall quality of sleep, based on various factors.</p>

<p>Using sleep data collected by the inbuilt accelerometer on supported iOS and Android devices, the developers of these apps are also able to provide extra functionality beyond tracking and analysis such as the ability for a user to be awoken gradually, using algorithms that factor in details about the user's sleep patterns.</p>

<p><a href="http://www.sleepcycle.com/">Sleep Cycle</a> for iOS does exactly this. The app knows when a user is in a state of 'light sleep', and is able to awaken them during one of these phases to ensure a fresher start to the day.</p>

<h2>Hypnopædia</h2>

<p>Hypnopædia, (or Hypnopaedia for those of us who prefer our letters disconnected from one-another) is the practice of conveying information to an individual whilst they sleep.
Supposedly, sleep-learning is moderately effective when subjects are fed direct passages or facts, word for word. Many scientists and researchers, however, disagree, with Charles W. Simon and William H. Emmons stating publicly that learning whilst asleep is <em>"impractical and probably impossible"</em>.</p>

<p><strong>Probably</strong> impossible. There's only one way to find out…</p>

<h2>Sleep cycles</h2>

<p>Taking a leaf out of the books of sleep cycle analysis app developers, Lucid will determine whether a subject is in R.E.M sleep (and therefore dreaming), or in a lighter state of sleep, possibly partially concious.</p>

<p>It is important for us to determine when our subject is in deep sleep, as this is the only time when we can be sure that they are truly unconcious. When in deep sleep (R.E.M), the body places itself in to a state of complete stillness, otherwise known as sleep paralysis. Luckily, this is relatively simple to monitor using the sensors available to us in modern mobile devices and will form the basis of our project.</p>

<h2>The Big Plan</h2>

<p><strong>The ultimate aim of our project is simple: to discover whether or not a subject is able to hear, understand, learn and be influenced by audio that is presented to them whilst in a sleeping state.</strong></p>

<p>Part game / part experiment, players will be presented with audio samples of differing qualities: simple, descriptive, musical etc. (these will be discussed in-depth in a future post).</p>

<p>The player's goal is to complete a number of puzzles in a 3D environment that require specific cues and clues to complete. The kicker? The clues are only presented to the player during sleep. Whoa.</p>

<p>A player is free to subject themselves to the audio every night, for as long as they would like, with only one opportunity to complete the puzzles once they have begun. It is up to a player to make the move from the 'training' phase to the 'test' phase when they feel confident enough, whether it be consciously or subconsciously.</p>


				</section>

			</article>

		

	

</ul>


		</section>

	</div>

	<footer class="global-footer" role="contentinfo">

		<div class="wrap"></div>

	</footer>

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
<script src="/lucid/js/app.js"></script>
</body>
</html>
